{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine learning is a fascinating field .\\nNatural language processing helps computers understand human language .\\nDeep learning models , such as neural networks , have revolutionized AI .\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = '''\n",
    "Machine learning is a fascinating field .\n",
    "Natural language processing helps computers understand human language .\n",
    "Deep learning models , such as neural networks , have revolutionized AI .\n",
    "'''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'machine': 1,\n",
       " 'learning': 2,\n",
       " 'is': 1,\n",
       " 'a': 1,\n",
       " 'fascinating': 1,\n",
       " 'field': 1,\n",
       " '.': 3,\n",
       " 'natural': 1,\n",
       " 'language': 2,\n",
       " 'processing': 1,\n",
       " 'helps': 1,\n",
       " 'computers': 1,\n",
       " 'understand': 1,\n",
       " 'human': 1,\n",
       " 'deep': 1,\n",
       " 'models': 1,\n",
       " ',': 2,\n",
       " 'such': 1,\n",
       " 'as': 1,\n",
       " 'neural': 1,\n",
       " 'networks': 1,\n",
       " 'have': 1,\n",
       " 'revolutionized': 1,\n",
       " 'ai': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = {}\n",
    "for sent in text.split('\\n'):\n",
    "    sent = [word.lower() for word in sent.split()]\n",
    "\n",
    "    for word in sent:\n",
    "        if word in vocab:\n",
    "            vocab[word] += 1\n",
    "        else:\n",
    "            vocab[word] = 1\n",
    "\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''\n",
    "machine\n",
    "learning\n",
    "learn\n",
    "learner\n",
    "revolutionized\n",
    "revolutionise\n",
    "revolutionary\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['machine',\n",
       " 'learning',\n",
       " 'learn',\n",
       " 'learner',\n",
       " 'revolutionized',\n",
       " 'revolutionise',\n",
       " 'revolutionary']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in text.strip().split('\\n')]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def charater_split(word):\n",
    "    # Use list comprehension to split the word into characters\n",
    "    characters = [char for char in word]\n",
    "    return characters\n",
    "\n",
    "splitting_word = [charater_split(word) for word in words]\n",
    "splitting_words = []\n",
    "for splitting in splitting_word:\n",
    "    splitting_words += splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'y',\n",
       " 'z'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set(splitting_words)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def calculate_pair_frequencies(lst):\n",
    "    # Initialize a dictionary to store pair frequencies\n",
    "    pair_freq = defaultdict(int)\n",
    "\n",
    "    # Iterate through the list and count pairs\n",
    "    for i in range(len(lst) - 1):\n",
    "        pair = (lst[i], lst[i+1])\n",
    "        pair_freq[pair] += 1\n",
    "\n",
    "    return pair_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_freq = calculate_pair_frequencies(splitting_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('a', 'r'), 4)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def find_max_pair_frequency(pair_freq):\n",
    "    if not pair_freq:\n",
    "        return None\n",
    "\n",
    "    max_pair = max(pair_freq, key=pair_freq.get)\n",
    "    max_frequency = pair_freq[max_pair]\n",
    "    \n",
    "    return max_pair, max_frequency\n",
    "\n",
    "max_pair, max_freq = find_max_pair_frequency(pair_freq)\n",
    "max_pair, max_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_max_pair(splitting_words, max_pair):\n",
    "    \n",
    "    new_splitting = []\n",
    "\n",
    "    idx_merged = []\n",
    "    # Iterate through the list and count pairs\n",
    "    for idx in range(len(splitting_words)):\n",
    "        if idx in idx_merged:\n",
    "            continue\n",
    "        if splitting_words[idx] == max_pair[0] and splitting_words[idx + 1] == max_pair[1]:\n",
    "            new_splitting.append(splitting_words[idx] + splitting_words[idx+1])\n",
    "            idx_merged.append(idx+1)\n",
    "        else:\n",
    "            new_splitting.append(splitting_words[idx])\n",
    "    return new_splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece(splitting_words):\n",
    "    pair_freq = calculate_pair_frequencies(splitting_words)\n",
    "\n",
    "    max_pair, _ = find_max_pair_frequency(pair_freq)\n",
    "\n",
    "    merged = merge_max_pair(splitting_words, max_pair)\n",
    "\n",
    "    return merged, max_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordpiece_n(splitting_words, n=9):\n",
    "    merged = splitting_words.copy()\n",
    "    rules = []\n",
    "    for _ in range(n):\n",
    "        merged, rule = wordpiece(merged)\n",
    "        print(merged)\n",
    "        rules.append(rule)\n",
    "\n",
    "    return merged, rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged1, rule = wordpiece(splitting_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged2 = wordpiece(merged1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'l', 'e', 'ar', 'n', 'i', 'n', 'g', 'l', 'e', 'ar', 'n', 'l', 'e', 'ar', 'n', 'e', 'r', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'le', 'ar', 'n', 'i', 'n', 'g', 'le', 'ar', 'n', 'le', 'ar', 'n', 'e', 'r', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'lear', 'n', 'i', 'n', 'g', 'lear', 'n', 'lear', 'n', 'e', 'r', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'r', 'e', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 're', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 're', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 're', 'v', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'rev', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'rev', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'rev', 'o', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revo', 'l', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'revo', 'l', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'revo', 'l', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revol', 'u', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'revol', 'u', 't', 'i', 'o', 'n', 'i', 's', 'e', 'revol', 'u', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revolu', 't', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'revolu', 't', 'i', 'o', 'n', 'i', 's', 'e', 'revolu', 't', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revolut', 'i', 'o', 'n', 'i', 'z', 'e', 'd', 'revolut', 'i', 'o', 'n', 'i', 's', 'e', 'revolut', 'i', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revoluti', 'o', 'n', 'i', 'z', 'e', 'd', 'revoluti', 'o', 'n', 'i', 's', 'e', 'revoluti', 'o', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revolutio', 'n', 'i', 'z', 'e', 'd', 'revolutio', 'n', 'i', 's', 'e', 'revolutio', 'n', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'i', 'n', 'e', 'learn', 'i', 'n', 'g', 'learn', 'learn', 'e', 'r', 'revolution', 'i', 'z', 'e', 'd', 'revolution', 'i', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'in', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolution', 'i', 'z', 'e', 'd', 'revolution', 'i', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['m', 'a', 'c', 'h', 'in', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['ma', 'c', 'h', 'in', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['mac', 'h', 'in', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['mach', 'in', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['machin', 'e', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n",
      "['machine', 'learn', 'in', 'g', 'learn', 'learn', 'e', 'r', 'revolutioni', 'z', 'e', 'd', 'revolutioni', 's', 'e', 'revolution', 'ar', 'y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['machine',\n",
       "  'learn',\n",
       "  'in',\n",
       "  'g',\n",
       "  'learn',\n",
       "  'learn',\n",
       "  'e',\n",
       "  'r',\n",
       "  'revolutioni',\n",
       "  'z',\n",
       "  'e',\n",
       "  'd',\n",
       "  'revolutioni',\n",
       "  's',\n",
       "  'e',\n",
       "  'revolution',\n",
       "  'ar',\n",
       "  'y'],\n",
       " [('a', 'r'),\n",
       "  ('l', 'e'),\n",
       "  ('le', 'ar'),\n",
       "  ('lear', 'n'),\n",
       "  ('r', 'e'),\n",
       "  ('re', 'v'),\n",
       "  ('rev', 'o'),\n",
       "  ('revo', 'l'),\n",
       "  ('revol', 'u'),\n",
       "  ('revolu', 't'),\n",
       "  ('revolut', 'i'),\n",
       "  ('revoluti', 'o'),\n",
       "  ('revolutio', 'n'),\n",
       "  ('i', 'n'),\n",
       "  ('revolution', 'i'),\n",
       "  ('m', 'a'),\n",
       "  ('ma', 'c'),\n",
       "  ('mac', 'h'),\n",
       "  ('mach', 'in'),\n",
       "  ('machin', 'e')])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpiece_n(splitting_words, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
